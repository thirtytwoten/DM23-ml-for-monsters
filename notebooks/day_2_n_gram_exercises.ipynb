{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04941e5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# N-gram Language Models\n",
    "\n",
    "\n",
    "N-gram language models are a way of predicting the next word based on n-1 preceding words.\n",
    "\n",
    "We will make our own n-gram language model from scratch, with the help of a few utilities from NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f64823",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/shannon_example_b.png\" alt=\"Drawing\" style=\"height: 250px;\"/><figcaption style=\"width: 280px;\">A language consisting only of the letters A, B, C, D, and E. Each letter has a probability.</figcaption></td>\n",
    "<td> <img src=\"../img/shannon_example_c.png\" alt=\"Drawing\" style=\"height: 250px;\"/><figcaption style=\"width: 350px;\">A language consisting only of the letters A, B, and C but with conditional transition probabilities.</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e32fb3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/onefish.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">A markov model of the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cc3a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/quicklyran.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">Markov model for generating a sentence of infinite possible length.</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811322d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Order\n",
    "\n",
    "The order of a Markov model is the number of previous states that the model takes into account. This model is second order, because we care about the previous two words when detexrmining the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44117a79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with a toy corpus of three sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed057f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = \"\"\"By this liberty they entered into a very laudable emulation to do all of them \\\n",
    "what they saw did please one. If any of the gallants or ladies should say, Let us drink, \\\n",
    "they would all drink.  If any one of them said, Let us play, they all played.  If one said, \\\n",
    "Let us go a-walking into the fields they went all.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd79ebb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "The first step of any text-based data project is usually pre-processing the text. \n",
    "\n",
    "**Tokenization:** Separate into contexts, words.\n",
    "\n",
    "**(Normalization):** Multiple spellings, capitalization.\n",
    "\n",
    "**(Lemmatization):** Different forms of a word collapse into one.\n",
    "\n",
    "**(Stop-word removal):** Discarding common words.\n",
    "\n",
    "**(Frequency Limits):** Discard very infrequent words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360fa16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise \n",
    "\n",
    "### 1: Tokenization\n",
    "\n",
    "Tokenize and the corpus and normalize it by making all the words the same case. That is, split the corpus into tokens and store them in a list. The output should look like a list of strings, e.g.\n",
    "\n",
    " ```\n",
    "       tokenized_corpus = [\"by\", \"this\", \"liberty\", ... \"all\", \".\"']\n",
    " ```\n",
    "\n",
    "You will use the `str.split()` method, the `str.lower()` or `str.upper()` methods. \n",
    "\n",
    "We also want punctuation to be its own token, which will take more than just splitting on whitespace.  If you like regexes, try building one that will separate punctuation from the preceding word, so that it registers as a separate token. You can use `re.sub()` for this. Consult the Python docs (these are your friend!) for information on these functions and to look for others that might help https://docs.python.org/3.11 . No shade for asking chatGPT to do this exercise for you, but dont ask it until you are sure youre ready to see how deep the rabbit hole goes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d35f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcda3d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# replace word+punctuation with word + space + punctuation\n",
    "# e.g. \"end.\" => \"end .\" \n",
    "# (\\w) = any word character\n",
    "# ([.,?!;:]) = any punctuation character\n",
    "# the parentheses define 'capture groups', and \\1 and \\2 refer back to the first and second capture groups\n",
    "tokenized_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', corpus) \n",
    "tokenized_corpus = tokenized_corpus.split()\n",
    "tokenized_corpus = [word.lower() for word in tokenized_corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cca270",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Get N-grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a7d99",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/fishbigrams.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\"> Bigrams for the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70191a3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/fishpairs.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">Bigrams for the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b4cda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise: Get N-grams and N-gram Frequencies\n",
    "\n",
    "Build a bigram conditional frequency distribution for each state given the last state. For this you will need to write two functions:\n",
    "\n",
    "   `ngrams(text,n)` should transform a text into a list of ngrams (n-token) spans in the text. For example, ngrams(tokenized_corpus, 2) would return something like :\n",
    "\n",
    "```\n",
    "    [('by', 'this'),\n",
    "     ('this', 'liberty'),\n",
    "     ('liberty', 'they'),\n",
    "     ('they', 'entered'),\n",
    "      ...\n",
    "    ]\n",
    "```\n",
    "\n",
    "If you are feeling stuck, try writing the function for bigrams first. That is, write it for just 2-word window size,\n",
    "\n",
    "   `ngram_frequency(ngram_list)` should read in the list of ngrams generated in the last step and construct a dictionary mapping from ngrams to integer counts, like:\n",
    "    \n",
    "```\n",
    "    {('by', 'this'): 1,\n",
    "     ('this', 'liberty'): 1,\n",
    "     ('liberty', 'they'): 1,\n",
    "      ...\n",
    "    }\n",
    "```\n",
    "\n",
    "This function should be more straightforward.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675ef00",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# get the ngrams for a corpus\n",
    "def ngrams(text, n):\n",
    "    raise Exception(\"not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe7085",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 2 # 2 for bigram 3 for trigram, etc\n",
    "ngram_list = ngrams(tokenized_corpus, n)\n",
    "ngram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed1760",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have generated  state/transition pairs which we formed by using a “window” to look at what the next token is in a pair. Pair are in the form\n",
    "    (current state, next word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b0d8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# calculate frequencies of the n-grams\n",
    "def ngram_frequency(ngram_list):\n",
    "    raise Exception(\"not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_frequencies = ngram_frequency(ngram_list)\n",
    "ngram_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98580160",
   "metadata": {},
   "source": [
    "# From Frequencies to Probabilities\n",
    "\n",
    "The next step is to take our frequency counts and convert them into probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08271966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives you the raw probabilities \n",
    "def ngram_probs(n_gram_frequencies, n):\n",
    "    probs = []\n",
    "    total = len(n_gram_frequencies)\n",
    "    for ngram_value, ngram_count in ngram_frequencies.items():\n",
    "        prob = ngram_count / total\n",
    "        probs.append([ngram_value, prob])\n",
    "    return probs\n",
    "\n",
    "# with laplace smoothing\n",
    "def ngram_probs_laplace_one(ngram_frequencies, n):\n",
    "    laplace_one_probs = []\n",
    "    lessgram = ngram_frequency(ngrams(tokenized_corpus, n-1))\n",
    "    vocabulary = len(ngram_frequencies)\n",
    "    for ngram_value, ngram_count in ngram_frequencies.items():\n",
    "        try:\n",
    "            prob = (ngram_count + 1) / (lessgram[ngram_value[:-1]] + vocabulary)\n",
    "            sum_prob = ++prob\n",
    "        except KeyError:\n",
    "            prob = (ngram_count + 1) / (bigram[\"UNK\"] + vocabulary)\n",
    "            sum_prob = ++math.log(prob)\n",
    "        laplace_one_probs.append([ngram_value, prob])\n",
    "    return laplace_one_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d541918",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ngram_probs(ngram_frequencies, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df165c5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ngram_probs_laplace_one(ngram_frequencies, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5874829",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We've seen the word let. what is the probability that us is the next word?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c0111",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "it's NOT .063. why is this? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa97cc",
   "metadata": {},
   "source": [
    "## An aside: data structures for counting words\n",
    "\n",
    "As an aside, first a few quick words about data structures in NLTK that support us in counting words (or word groups, or pieces of syntactic structure). The first is basically a dictionary mapping words to counts, called a FreqDist. Conveniently, you can just initialize it by giving it a list of items, and it will count how often each item appears in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK data structures for counting stuff:\n",
    "# count individual words or other items:\n",
    "import nltk\n",
    "\n",
    "fd = nltk.FreqDist([\"a\", \"b\", \"c\", \"a\", \"b\", \"d\"])\n",
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabe47a",
   "metadata": {},
   "source": [
    "The second data structure relevant for us today is the ConditionalFreqDist. It also has counts, but it can be used to count, for each target, how often each context word appears, or more generally, how often each word appears given some other word. Say \"a\" is a target, and \"b\" and \"c\" are context items, then a ConditionalFreqDist can be used like a two-deep dictionary, whose first-level keys are called \"conditions\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f683b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for targets, count context words,\n",
    "# or in general, for one sort of items, \n",
    "# count another sort of items\n",
    "cfd = nltk.ConditionalFreqDist()\n",
    "cfd[\"a\"][\"b\"] += 1\n",
    "cfd[\"a\"][\"c\"] += 1\n",
    "cfd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fe8c3",
   "metadata": {},
   "source": [
    "For the \"condition\" 'a', the entry is again a FreqDist object that counts appearances of 'b' and 'c':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a210a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1918be4",
   "metadata": {},
   "source": [
    "You can also initialize a ConditionalFreqDist by a list of pairs. It then counts, for each first item of the pair, how often each second item appears. In the next example, the ConditionalFreqDist will record that given \"a\", both \"b\" and \"c\" appeared once, and that given \"d\", \"e\" appeared once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a72a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist([(\"a\", \"b\"), (\"a\", \"c\"), (\"d\", \"e\")])\n",
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ab893",
   "metadata": {},
   "source": [
    "# Back to our corpus\n",
    "## Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f6e5af",
   "metadata": {},
   "source": [
    "We then organize these pairs by the current state. We match every state to the number of possible ways to transition out of that state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d93899",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/fishprobs.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">A markov model of the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist\n",
    "\n",
    "cfd = ConditionalFreqDist(ngram_list)\n",
    "cfd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2cac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "\n",
    "cpd = ConditionalProbDist(cfd, ELEProbDist, 10)\n",
    "cpd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d03409",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's look at one of the probability distributions\n",
    "\n",
    "cpd['one'].samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35eea9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# this class comes with a generate function that makes a random sample according to the probability distirbution, \n",
    "# like rolling a weighted die\n",
    "cpd['one'].generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e76ad10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<table><tr>\n",
    "<td> <img src=\"../img/fishmarkov.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">A markov model of the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n",
    "</tr></table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8a6ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<td> <img src=\"../img/fishmarkovweights.png\" alt=\"Drawing\" style=\"height: 400px;\"/><figcaption style=\"width: 280px;\">A markov model of the sentence \"one fish two fish red fish blue fish\".</figcaption></td>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd93f2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: text generation with an n-gram model\n",
    "\n",
    "Write a function called generate() that generates sentences using the model. It takes as input our bigram conditional probability distribution and a 'num_sentences' parameter. It uses the `ConditionalFreqDist.generate()` method to generate one token at a time, conditioned on the last token it generated.\n",
    "\n",
    "Questions to think about: How should each sentence start? How will you know when you are done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948bc93",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def generate(cpdist, num_sentences=10, seed=None):\n",
    "    \"\"\"\n",
    "    model is an nltk ConditionalProbDist\n",
    "    length is the number of tokens we wish to generate.\n",
    "    \"\"\"\n",
    "\n",
    "    # to add to\n",
    "    string = []\n",
    "    \n",
    "    \n",
    "    seed = cpd['.'].generate()\n",
    "    string.append(seed)\n",
    "    lessgram=seed\n",
    "    \n",
    "    \n",
    "    for i in range(num_sentences):\n",
    "    # start by sampling a word that comes after a period\n",
    "        while True:\n",
    "            next_token = cpd[lessgram].generate()\n",
    "            string.append(next_token)\n",
    "            lessgram = string[-1]\n",
    "\n",
    "            if next_token == '.':\n",
    "                break\n",
    "    return ' '.join(string)\n",
    "\n",
    "generate(cpd, num_sentences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d347a",
   "metadata": {},
   "source": [
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n",
    "# Don't PEEK!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2dab4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Putting it all together\n",
    "\n",
    "Let's put some of the methods together in a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab34db3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class BigramModel():\n",
    "    from nltk import ConditionalFreqDist\n",
    "    from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        n = 2\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _tokenize(self, corpus):\n",
    "        tokenized_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', corpus) \n",
    "        tokenized_corpus = tokenized_corpus.split()\n",
    "        tokenized_corpus = [word.lower() for word in tokenized_corpus]\n",
    "        return tokenized_corpus\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))\n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "        cfd = ConditionalFreqDist(self._ngrams)\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, 10)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "        \n",
    "    def generate(self, num_sentences=10, seed=None):\n",
    "        string = []\n",
    "        \n",
    "        seed = self._cpd['.'].generate()\n",
    "        string.append(seed)\n",
    "        lessgram=seed\n",
    "\n",
    "        for i in range(num_sentences):\n",
    "        # start by sampling a word that comes after a period\n",
    "            while True:\n",
    "                next_token = self._cpd[lessgram].generate()\n",
    "                string.append(next_token)\n",
    "                lessgram = string[-1]\n",
    "\n",
    "                if next_token == '.':\n",
    "                    break\n",
    "        return ' '.join(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148c9e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = BigramModel(corpus)\n",
    "\n",
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16adc558",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalizing the bigram model\n",
    "\n",
    "The bigram model is cool, but it still generates very unusial structures like ''they would all of the fields''. While this is beautiful, we might be after something a little closer to 'passing' as language. \n",
    "\n",
    "This can be achieved by conditioning on more than one previous word at a time. So, instead of just seeing 'of', the model maintains state for the last two tokens seen: 'all of'. This reduces the number of \n",
    "\n",
    "In fact, it would be great if we could generalize our model for arbitrary n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59325081",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Write a function that will build an n-gram conditional probability distribution.\n",
    "\n",
    "**Hint:** the ConditionalFrequencyDistribution class can also be instantiated empty like this\n",
    "\n",
    "`cfd = ConditionalFreqDist()`\n",
    "\n",
    "You can add frequencies for conditions by accessing them like a dictionary, and they'll be created automatically if they don't already exist.\n",
    "Like this: \n",
    "\n",
    "`cfd[\"a\"][\"b\"] += 1`\n",
    "\n",
    "This will add one to the frequency count for the b outcome of the a condition. Conditions don't have to be strings but can be more complex objects like tuples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    " def _build_distribution(self, corpus, n):\n",
    "    # we have to transform the ngram list to be priors and possible outcomes.\n",
    "    # how do you split up a trigram? a 4gram?\n",
    "    cpd = None\n",
    "    self.cpd = cpd\n",
    "    return cpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf54143",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can rewrite the BigramModel class to construct an n-gram model for an arbitrary n. The class constructor takes n as an input in addition to the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be98f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "\n",
    "class NgramModel():\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, n):\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self.n = n\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _tokenize(self, corpus):\n",
    "        tokenized_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', corpus) \n",
    "        tokenized_corpus = tokenized_corpus.split()\n",
    "        tokenized_corpus = [word.lower() for word in tokenized_corpus]\n",
    "        \n",
    "        # add pad tokens to the corpus\n",
    "        tokenized_corpus = list(pad_both_ends(tokenized_corpus, n=2))\n",
    "        return tokenized_corpus\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))\n",
    "        return n_grams\n",
    "    \n",
    "        print(n_grams)\n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "        raise Exception(\"not implemented\")\n",
    "        \n",
    "        self.cpd = None\n",
    "        return cpd\n",
    "        \n",
    "        \n",
    "    def generate(self, num_tokens = 20, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram.\n",
    "        \"\"\"\n",
    "        string = []\n",
    "        \n",
    "    \n",
    "        seed = self.cpd['.'].generate()\n",
    "        string.append(seed)\n",
    "        lessgram=seed\n",
    "\n",
    "        for i in range(num_tokens):\n",
    "        # start by sampling a word that comes after a period\n",
    "            next_token = cpdist[lessgram].generate()\n",
    "            string.append(next_token)\n",
    "            lessgram = string[-1]\n",
    "            \n",
    "        return ' '.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61138967",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = NgramModel(corpus, 3)\n",
    "model._cpd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f1078",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = NgramModel(corpus, 4)\n",
    "model._cpd.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b826138",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But there is a problem. The generate function won't work! We need to extend it to work for arbitrary n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a76787",
   "metadata": {},
   "source": [
    "## Exercise (if time)\n",
    "\n",
    "Implement a generate function for our arbitrary n-gram model class. It should take as input a keyword argument `num_sentences` with the number of sentences to generate. Optionally, you can write it with a `seed` keyword argument that has an initial input to the model. If no input is given, the model can't condition on the previous n-1 grams! What should it start with? One trick for dealing with this is to take the input string and append start tokens to the front of it, so there are always enough tokens to condition on. You can use the `pad_sequence` function from NLTK for this. Otherwise, you will have to deal with the start case separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "\n",
    "\n",
    "def generate(self, num_sentences = 1, seed = []):\n",
    "    \"\"\"\n",
    "    There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "    If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "    If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "    We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "    We can use a unigram model! \n",
    "    Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "    And how can we \n",
    "    Either way, we need a seed condition to enter into the loop with.\n",
    "    \"\"\"\n",
    "\n",
    "    # place to put generated tokens\n",
    "    string = []\n",
    "\n",
    "    if seed:\n",
    "        string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "    else:\n",
    "        string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "\n",
    "    for i in range(num_sentences):\n",
    "        next_token = tuple(string[-(self.n-1):])\n",
    "\n",
    "        # keep generating tokens as long as we havent reached the stop sequence\n",
    "        while next_token != '</s>':\n",
    "\n",
    "            # get the last n-1 tokens to condition on next\n",
    "            lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "\n",
    "            next_token = self.cpd[lessgram].generate()\n",
    "            string.append( next_token )\n",
    "\n",
    "    string = ' '.join(string)\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(corpus, 3)\n",
    "\n",
    "#print(model.cpd.items())\n",
    "#print(model.cpd[('<s>', '<s>')].generate())\n",
    "\n",
    "model.generate(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
